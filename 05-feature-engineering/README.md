# Module 5: Feature Engineering for ML/AI

**Goal**: Transform raw data into features that make ML models work better.

**Key Insight**: "Garbage in, garbage out." Good features > Complex models.

## ðŸ“š What You'll Learn

### Core Techniques
- Numerical features (scaling, binning, transformations)
- Categorical encoding (one-hot, label, target)
- Text features (TF-IDF, embeddings)
- Date/time features (extract temporal patterns)
- Feature interactions (combine features)
- Dimensionality reduction (PCA, feature selection)

### Advanced Techniques
- Handling missing data (imputation strategies)
- Outlier treatment (cap, remove, transform)
- Feature creation from domain knowledge
- Automated feature engineering (Featuretools)
- Feature importance analysis

## ðŸŽ¯ Real-World Applications

- **E-commerce**: Customer lifetime value, purchase patterns
- **Finance**: Risk scoring, fraud detection
- **Healthcare**: Disease prediction, patient outcomes
- **NLP**: Text classification, sentiment analysis
- **Computer Vision**: Image preprocessing, augmentation
- **Time Series**: Forecasting, anomaly detection

## ðŸ“‚ Module Structure

```
05-feature-engineering/
â”œâ”€â”€ README.md (you are here)
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ 01_numerical_features.py     # Scaling, binning, transforms
â”‚   â”œâ”€â”€ 02_categorical_encoding.py   # Encoding techniques
â”‚   â”œâ”€â”€ 03_text_features.py          # TF-IDF, embeddings
â”‚   â”œâ”€â”€ 04_datetime_features.py      # Temporal patterns
â”‚   â”œâ”€â”€ 05_feature_interactions.py   # Combining features
â”‚   â”œâ”€â”€ 06_dimensionality_reduction.py  # PCA, feature selection
â”‚   â””â”€â”€ 07_complete_pipeline.py      # End-to-end example
â””â”€â”€ mini_project/
    â””â”€â”€ customer_churn_prediction.py # Real ML project
```

## ðŸ’¡ Feature Engineering Principles

**1. Domain Knowledge Beats Algorithms**
- Understanding your data > fancy techniques
- Business logic â†’ better features

**2. Start Simple, Add Complexity**
- Basic features first
- Test impact before adding more

**3. Avoid Data Leakage**
- Don't use future information
- Fit on train, transform on test

**4. Feature Quality > Quantity**
- 10 good features > 100 mediocre ones
- Remove redundant/correlated features

## ðŸ”§ Common Feature Types

**Numerical**:
- Continuous: age, price, distance
- Discrete: count, rating, rank

**Categorical**:
- Nominal: color, category, country
- Ordinal: rating, education level

**Text**:
- Short: product names, tags
- Long: reviews, descriptions, documents

**DateTime**:
- Timestamps: order_date, login_time
- Durations: session_length, days_since

**Derived**:
- Ratios: price_per_sqft
- Aggregates: avg_purchase_last_30_days
- Interactions: age * income

## ðŸŽ¯ Feature Engineering Workflow

```
Raw Data
   â†“
1. Understand Data (EDA)
   â†“
2. Handle Missing Values
   â†“
3. Encode Categoricals
   â†“
4. Scale Numericals
   â†“
5. Create New Features
   â†“
6. Select Best Features
   â†“
ML-Ready Data
```

## ðŸ“Š Impact on Model Performance

Example: House Price Prediction

| Features | RÂ² Score |
|----------|----------|
| Raw features (5) | 0.65 |
| + Scaling | 0.70 |
| + Polynomial features | 0.78 |
| + Domain features | 0.85 |
| + Feature selection | 0.87 |

**5% improvement** from good feature engineering!

## ðŸš€ Quick Start

```bash
# Install dependencies
poetry add numpy pandas scikit-learn

# Start with numerical features
poetry run python 05-feature-engineering/examples/01_numerical_features.py

# Try the complete pipeline
poetry run python 05-feature-engineering/examples/07_complete_pipeline.py
```

---

**Remember**: Better features make simple models outperform complex ones! ðŸŽ¯
