# Module 8: LLMs and Embeddings

**Goal**: Work with Large Language Models and vector embeddings for modern AI applications.

**Focus**: Practical usage of OpenAI, Anthropic, and open-source LLMs.

## ğŸ“š What You'll Learn

### LLM Fundamentals
- How LLMs work (transformers, attention)
- Prompting techniques (zero-shot, few-shot, CoT)
- Temperature, top-p, and other parameters
- Token management and costs
- Context windows and limitations

### Working with APIs
- OpenAI API (GPT-4, GPT-3.5)
- Anthropic API (Claude)
- Open-source models (Llama, Mistral)
- Streaming responses
- Function calling
- Error handling and retries

### Embeddings
- What are embeddings? (vector representations)
- Text embeddings (OpenAI, Sentence Transformers)
- Similarity search (cosine similarity)
- Semantic search
- Clustering documents
- Recommendation systems

### Vector Databases
- ChromaDB (local, easy)
- Pinecone (cloud, scalable)
- Weaviate (hybrid search)
- FAISS (Facebook's similarity search)
- Storing and querying embeddings

### Advanced Techniques
- Prompt engineering
- Prompt templates
- Output parsing
- Chaining LLM calls
- Agents and tools
- Fine-tuning (basics)

## ğŸ¯ Real-World Applications

- **Semantic Search**: Find similar documents
- **Question Answering**: Over your own data
- **Chatbots**: Customer support, assistants
- **Content Generation**: Articles, emails, code
- **Summarization**: Long documents â†’ summaries
- **Classification**: Zero-shot text classification
- **Recommendation**: Content/product recommendations

## ğŸ“‚ Module Structure

```
08-llms-and-embeddings/
â”œâ”€â”€ README.md (you are here)
â”œâ”€â”€ fundamentals/
â”‚   â”œâ”€â”€ 01_openai_basics.py          # First API call
â”‚   â”œâ”€â”€ 02_prompt_engineering.py     # Effective prompts
â”‚   â”œâ”€â”€ 03_function_calling.py       # Tool usage
â”‚   â”œâ”€â”€ 04_streaming.py              # Real-time responses
â”‚   â””â”€â”€ 05_error_handling.py         # Robust API calls
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ 01_text_embeddings.py        # Generate embeddings
â”‚   â”œâ”€â”€ 02_similarity_search.py      # Find similar text
â”‚   â”œâ”€â”€ 03_clustering.py             # Group documents
â”‚   â””â”€â”€ 04_semantic_search.py        # Better than keyword search
â”œâ”€â”€ vector_databases/
â”‚   â”œâ”€â”€ 01_chromadb_basics.py        # Local vector DB
â”‚   â”œâ”€â”€ 02_pinecone_basics.py        # Cloud vector DB
â”‚   â”œâ”€â”€ 03_qdrant_basics.py          # Alternative
â”‚   â””â”€â”€ 04_comparison.py             # When to use which
â”œâ”€â”€ advanced/
â”‚   â”œâ”€â”€ 01_prompt_templates.py       # Reusable prompts
â”‚   â”œâ”€â”€ 02_output_parsing.py         # Structured outputs
â”‚   â”œâ”€â”€ 03_chaining_calls.py         # Multi-step reasoning
â”‚   â””â”€â”€ 04_agents.py                 # Autonomous agents
â””â”€â”€ projects/
    â”œâ”€â”€ semantic_search_engine/      # Search your docs
    â”œâ”€â”€ chatbot/                     # Q&A over docs
    â””â”€â”€ content_generator/           # Automated writing
```

## ğŸ’¡ Key Concepts

### Embeddings Intuition
```
Text â†’ Vector (list of numbers)

"cat" â†’ [0.2, 0.8, 0.1, ...]
"dog" â†’ [0.3, 0.7, 0.1, ...]  â† Similar to "cat"
"car" â†’ [0.8, 0.1, 0.9, ...]  â† Different from "cat"

Similar meaning â†’ Similar vectors
```

### Similarity Search
```
1. Convert all documents to embeddings
2. Store in vector database
3. Convert query to embedding
4. Find nearest neighbors
5. Return most similar documents
```

### LLM vs Traditional ML

| Task | Traditional ML | LLM |
|------|---------------|-----|
| Training Data | Thousands of examples | Zero/few examples |
| Training Time | Hours/days | Already trained |
| Task Specific | Need to retrain | Just change prompt |
| Cost | Training cost | API call cost |

## ğŸ”§ API Keys Setup

```bash
# Set environment variables
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export PINECONE_API_KEY="..."

# Or use .env file
echo "OPENAI_API_KEY=sk-..." > .env
```

## ğŸ“Š Cost Considerations

**OpenAI Pricing (as of 2024)**:
- GPT-4: $0.03/1K input tokens, $0.06/1K output tokens
- GPT-3.5 Turbo: $0.0005/1K input tokens, $0.0015/1K output tokens
- Embeddings: $0.0001/1K tokens

**Tips**:
- Use GPT-3.5 for simple tasks
- Cache embeddings (generate once, use many times)
- Set max_tokens to limit costs
- Use streaming for better UX (doesn't save cost)

## ğŸ“ Prompting Best Practices

**1. Be Specific**
```
âŒ "Write about AI"
âœ… "Write a 200-word introduction to neural networks for beginners"
```

**2. Provide Context**
```
âœ… "You are an expert Python developer. Review this code for bugs:
    [code here]"
```

**3. Use Examples (Few-shot)**
```
âœ… "Classify sentiment:
    'I love this!' â†’ Positive
    'This is terrible' â†’ Negative
    'It was okay' â†’ Neutral
    'This is amazing!' â†’ ?"
```

**4. Chain of Thought**
```
âœ… "Let's think step by step:
    1. First, identify the problem
    2. Then, list possible solutions
    3. Finally, recommend the best approach"
```

## ğŸš€ Quick Start

```bash
# Install dependencies
poetry add openai anthropic chromadb sentence-transformers

# Set API key
export OPENAI_API_KEY="your-key-here"

# First API call
poetry run python 08-llms-and-embeddings/fundamentals/01_openai_basics.py

# Semantic search
poetry run python 08-llms-and-embeddings/embeddings/04_semantic_search.py

# Vector database
poetry run python 08-llms-and-embeddings/vector_databases/01_chromadb_basics.py
```

## ğŸ¯ Learning Path

1. **LLM Basics** â†’ Make API calls, understand parameters
2. **Prompt Engineering** â†’ Get better results
3. **Embeddings** â†’ Convert text to vectors
4. **Similarity Search** â†’ Find related content
5. **Vector Databases** â†’ Store and query at scale
6. **Advanced** â†’ Agents, chaining, structured outputs
7. **Projects** â†’ Build real applications

## âš ï¸ Common Pitfalls

1. **Not handling rate limits** â†’ Add retries
2. **Ignoring costs** â†’ Monitor token usage
3. **Poor prompt engineering** â†’ Iterate on prompts
4. **Not caching embeddings** â†’ Expensive!
5. **Hallucinations** â†’ Verify important facts

## ğŸ¯ Expected Outcomes

After this module:
- âœ… Call LLM APIs effectively
- âœ… Write effective prompts
- âœ… Generate and use embeddings
- âœ… Build semantic search
- âœ… Use vector databases
- âœ… Create LLM-powered applications
- âœ… Understand costs and limitations

---

**Next**: Module 9 - RAG (Retrieval Augmented Generation) ğŸš€
