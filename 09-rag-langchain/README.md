# Module 9: RAG and LangChain

**Goal**: Build Retrieval Augmented Generation (RAG) systems for Q&A over your own data.

**Why RAG?** LLMs don't know your proprietary data. RAG connects LLMs to your documents.

## ğŸ“š What You'll Learn

### RAG Fundamentals
- What is RAG? (Retrieval + Generation)
- Why use RAG? (Fresh data, reduced hallucinations)
- RAG architecture (document â†’ chunks â†’ embeddings â†’ retrieval â†’ generation)
- Document loaders (PDF, Markdown, web, APIs)
- Text chunking strategies
- Retrieval methods

### LangChain Framework
- Core concepts (chains, agents, memory)
- Document loaders and splitters
- Vector stores integration
- Prompt templates
- Output parsers
- Memory (conversation history)
- Agents and tools

### Vector Store Operations
- Ingestion pipeline (load â†’ split â†’ embed â†’ store)
- Retrieval strategies (similarity, MMR, threshold)
- Metadata filtering
- Hybrid search (keyword + semantic)
- Re-ranking results

### Advanced RAG
- Query expansion
- Hypothetical document embeddings (HyDE)
- Multi-query retrieval
- Parent document retrieval
- Ensemble retrievers
- RAG evaluation (faithfulness, relevancy)

### Production Considerations
- Caching strategies
- Error handling
- Monitoring and logging
- Cost optimization
- Scaling vector databases
- A/B testing RAG pipelines

## ğŸ¯ Real-World Applications

- **Documentation Q&A**: Ask questions about docs
- **Customer Support**: Answer from knowledge base
- **Research Assistant**: Query research papers
- **Code Q&A**: Ask about codebases
- **Legal/Compliance**: Query regulations
- **Internal Knowledge**: Company wiki Q&A

## ğŸ“‚ Module Structure

```
09-rag-langchain/
â”œâ”€â”€ README.md (you are here)
â”œâ”€â”€ fundamentals/
â”‚   â”œâ”€â”€ 01_langchain_basics.py       # First chain
â”‚   â”œâ”€â”€ 02_document_loaders.py       # Load various formats
â”‚   â”œâ”€â”€ 03_text_splitting.py         # Chunk strategies
â”‚   â”œâ”€â”€ 04_embeddings.py             # Generate embeddings
â”‚   â””â”€â”€ 05_vector_stores.py          # Store & retrieve
â”œâ”€â”€ rag_basics/
â”‚   â”œâ”€â”€ 01_simple_rag.py             # Basic RAG pipeline
â”‚   â”œâ”€â”€ 02_conversational_rag.py     # With memory
â”‚   â”œâ”€â”€ 03_metadata_filtering.py     # Filter by metadata
â”‚   â””â”€â”€ 04_hybrid_search.py          # Keyword + semantic
â”œâ”€â”€ advanced_rag/
â”‚   â”œâ”€â”€ 01_query_expansion.py        # Multiple queries
â”‚   â”œâ”€â”€ 02_hyde.py                   # Hypothetical docs
â”‚   â”œâ”€â”€ 03_reranking.py              # Improve results
â”‚   â”œâ”€â”€ 04_parent_retriever.py       # Hierarchical chunks
â”‚   â””â”€â”€ 05_agent_rag.py              # Agents for RAG
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ 01_faithfulness.py           # Answer accuracy
â”‚   â”œâ”€â”€ 02_relevancy.py              # Retrieval quality
â”‚   â”œâ”€â”€ 03_groundedness.py           # Source attribution
â”‚   â””â”€â”€ 04_ragas_framework.py        # Full evaluation
â””â”€â”€ projects/
    â”œâ”€â”€ documentation_qa/            # Ask about docs
    â”œâ”€â”€ research_assistant/          # Query papers
    â””â”€â”€ customer_support/            # Knowledge base Q&A
```

## ğŸ’¡ RAG Architecture

```
User Query
   â†“
Query Embedding
   â†“
Vector Similarity Search
   â†“
Retrieve Top K Documents
   â†“
Construct Prompt (query + retrieved docs)
   â†“
LLM Generation
   â†“
Answer + Sources
```

## ğŸ”§ RAG Pipeline Steps

**1. Ingestion (offline)**
```python
# Load documents
docs = load_documents("data/")

# Split into chunks
chunks = text_splitter.split_documents(docs)

# Generate embeddings
embeddings = embed_documents(chunks)

# Store in vector DB
vector_store.add_documents(chunks)
```

**2. Retrieval (online)**
```python
# User asks question
query = "What is RAG?"

# Retrieve relevant chunks
docs = vector_store.similarity_search(query, k=4)

# Generate answer
answer = llm.generate(query, context=docs)
```

## ğŸ“Š Chunking Strategies

| Strategy | When to Use | Pros | Cons |
|----------|-------------|------|------|
| Fixed Size | General purpose | Simple | May split sentences |
| Sentence | Semantic units | Preserves meaning | Variable size |
| Paragraph | Natural boundaries | Contextual | May be too large |
| Recursive | Hierarchical | Smart splitting | More complex |
| Semantic | Similar content | Coherent chunks | Slow |

**Recommended**: Start with Recursive (500 tokens, 50 overlap)

## ğŸ“ LangChain Components

**Chains**: Sequence of operations
```python
chain = prompt | llm | parser
result = chain.invoke({"question": "What is RAG?"})
```

**Agents**: Autonomous decision-makers
```python
agent = create_agent(llm, tools=[search, calculator])
agent.run("What's 20% of the GDP of France?")
```

**Memory**: Conversation history
```python
memory = ConversationBufferMemory()
chain = ConversationalRetrievalChain(llm, retriever, memory)
```

## ğŸš€ Quick Start

```bash
# Install dependencies
poetry add langchain langchain-openai chromadb pypdf

# Set API key
export OPENAI_API_KEY="your-key-here"

# Simple RAG
poetry run python 09-rag-langchain/rag_basics/01_simple_rag.py

# With conversation memory
poetry run python 09-rag-langchain/rag_basics/02_conversational_rag.py

# Full project
poetry run python 09-rag-langchain/projects/documentation_qa/app.py
```

## ğŸ’° Cost Optimization

**1. Cache Embeddings**
```python
# Generate once, use many times
embeddings = cache.get_or_create(document)
```

**2. Smaller Chunks**
```python
# Less context to LLM = lower cost
# But maintain semantic coherence
```

**3. Use Cheaper LLM for Retrieval**
```python
# GPT-3.5 for initial filtering
# GPT-4 for final generation
```

**4. Implement Caching**
```python
# Cache common questions
if question in cache:
    return cache[question]
```

## ğŸ¯ RAG Best Practices

**1. Chunk Size Matters**
- Too small: Loss of context
- Too large: Irrelevant info, high cost
- Sweet spot: 500-1000 tokens

**2. Include Overlap**
- 10-20% overlap between chunks
- Prevents cutting sentences/concepts

**3. Use Metadata**
- Source, date, author, section
- Filter retrieval by metadata

**4. Re-rank Results**
- Initial retrieval: Top 20
- Re-rank to Top 5
- Send only best to LLM

**5. Cite Sources**
- Always return source documents
- Users can verify answers

**6. Handle "I don't know"**
- If no relevant docs, admit it
- Don't hallucinate

## âš ï¸ Common Pitfalls

1. **Poor chunking** â†’ Irrelevant context
2. **No metadata filtering** â†’ Wrong documents
3. **Not citing sources** â†’ Can't verify
4. **Too many chunks to LLM** â†’ High cost, slow
5. **No conversation memory** â†’ Loses context
6. **Not handling edge cases** â†’ Bad UX

## ğŸ¯ Expected Outcomes

After this module:
- âœ… Build RAG systems with LangChain
- âœ… Load and process documents
- âœ… Implement effective chunking
- âœ… Use vector stores efficiently
- âœ… Handle conversations with memory
- âœ… Evaluate RAG performance
- âœ… Deploy production RAG apps

---

**Next**: Module 10 - Full-Stack AI Applications ğŸš€
