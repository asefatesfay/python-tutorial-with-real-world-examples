"""
Performance and Load Testing

Learn how to test ML systems under load and measure performance.
Focus: Response time, throughput, resource usage, stress testing.

Install: poetry add --group dev pytest pytest-benchmark
Run: pytest integration_tests/04_performance_tests.py -v
"""

import pytest
import time
import numpy as np
import pandas as pd
from typing import List
import concurrent.futures
import psutil
import os


# ============================================================================
# 1. Why Performance Testing?
# ============================================================================

def demo_why_performance_testing():
    """
    Why performance testing is critical for ML systems.
    """
    print("=" * 70)
    print("1. Why Performance Testing?")
    print("=" * 70)
    print()
    
    print("ðŸ’¥ REAL-WORLD PERFORMANCE DISASTERS:")
    print()
    print("   Story 1: The Slow Endpoint")
    print("   â€¢ Development: Responses in 100ms âœ…")
    print("   â€¢ Production: Responses in 30 seconds ðŸ’€")
    print("   â€¢ Cause: Model loaded on every request")
    print("   â€¢ Impact: Timeouts, angry users")
    print("   â€¢ Solution: Load model once, cache")
    print()
    print("   Story 2: The Memory Leak")
    print("   â€¢ Development: 500MB memory usage")
    print("   â€¢ Production: 32GB â†’ Crashes every 6 hours ðŸ’€")
    print("   â€¢ Cause: Not releasing tensors after prediction")
    print("   â€¢ Impact: Frequent restarts, downtime")
    print("   â€¢ Solution: Proper resource cleanup")
    print()
    print("   Story 3: The Bottleneck")
    print("   â€¢ Load test: 10 users â†’ Works fine")
    print("   â€¢ Production: 1000 users â†’ 95% failures ðŸ’€")
    print("   â€¢ Cause: Single-threaded processing")
    print("   â€¢ Impact: Black Friday disaster")
    print("   â€¢ Solution: Async processing, load balancing")
    print()
    
    print("ðŸŽ¯ WHY PERFORMANCE MATTERS:")
    print()
    print("   User expectations:")
    print("   â€¢ < 100ms: Instant (feels immediate)")
    print("   â€¢ < 1s: Fast (acceptable for search)")
    print("   â€¢ < 3s: Slow (user gets impatient)")
    print("   â€¢ > 3s: Very slow (user leaves)")
    print()
    print("   Business impact:")
    print("   â€¢ Amazon: 100ms delay â†’ 1% sales loss")
    print("   â€¢ Google: 500ms delay â†’ 20% traffic loss")
    print("   â€¢ Your ML API: Slow â†’ Users leave")
    print()
    
    print("ðŸ’° ROI OF PERFORMANCE TESTING:")
    print()
    print("   Without performance tests:")
    print("   â€¢ Production outage: 4 hours")
    print("   â€¢ Lost revenue: $50,000")
    print("   â€¢ Emergency scaling: $10,000/month")
    print()
    print("   With performance tests:")
    print("   â€¢ Catch before deploy")
    print("   â€¢ Optimize proactively")
    print("   â€¢ Right-size infrastructure")
    print("   â€¢ Lost revenue: $0 âœ…")
    print()


# ============================================================================
# 2. Simple ML Service to Test
# ============================================================================

class SimpleMLService:
    """Simple ML service for performance testing."""
    
    def __init__(self):
        self.model_weights = np.random.randn(1000, 100)  # Simulate model
        self.cache = {}
    
    def predict(self, features: List[float]) -> float:
        """Make prediction."""
        # Simulate computation
        features_array = np.array(features)
        result = np.dot(self.model_weights[0], features_array)
        return float(result)
    
    def predict_batch(self, features_list: List[List[float]]) -> List[float]:
        """Batch prediction."""
        return [self.predict(f) for f in features_list]
    
    def predict_with_cache(self, features: List[float]) -> float:
        """Prediction with caching."""
        cache_key = tuple(features)
        
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        result = self.predict(features)
        self.cache[cache_key] = result
        return result


# ============================================================================
# 3. Response Time Testing
# ============================================================================

@pytest.fixture
def ml_service():
    """Fixture providing ML service."""
    return SimpleMLService()


def test_single_prediction_response_time(ml_service):
    """Test single prediction response time."""
    features = [0.5] * 100
    
    start_time = time.time()
    ml_service.predict(features)
    elapsed = time.time() - start_time
    
    # Should be less than 10ms
    assert elapsed < 0.01, f"Too slow: {elapsed*1000:.2f}ms"


def test_batch_prediction_response_time(ml_service):
    """Test batch prediction response time."""
    batch_features = [[0.5] * 100 for _ in range(100)]
    
    start_time = time.time()
    ml_service.predict_batch(batch_features)
    elapsed = time.time() - start_time
    
    # 100 predictions should be less than 1 second
    assert elapsed < 1.0, f"Batch too slow: {elapsed:.2f}s"


def test_cached_prediction_faster(ml_service):
    """Test that cached predictions are faster."""
    features = [0.5] * 100
    
    # First call (not cached)
    start_time = time.time()
    ml_service.predict_with_cache(features)
    first_call_time = time.time() - start_time
    
    # Second call (cached)
    start_time = time.time()
    ml_service.predict_with_cache(features)
    second_call_time = time.time() - start_time
    
    # Cached should be at least 2x faster
    assert second_call_time < first_call_time / 2


@pytest.mark.parametrize("n_features", [10, 50, 100, 500])
def test_prediction_scales_with_features(ml_service, n_features):
    """Test that prediction time scales reasonably with feature count."""
    features = [0.5] * n_features
    
    start_time = time.time()
    ml_service.predict(features)
    elapsed = time.time() - start_time
    
    # Should still be fast even with many features
    assert elapsed < 0.1, f"Too slow with {n_features} features: {elapsed*1000:.2f}ms"


def demo_response_time_testing():
    """Demo response time testing."""
    print("\n" + "=" * 70)
    print("2. Response Time Testing")
    print("=" * 70)
    print()
    
    print("â±ï¸  RESPONSE TIME TARGETS:")
    print()
    print("   Operation              Target      Max")
    print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print("   Single prediction      < 10ms      50ms")
    print("   Batch (100)            < 100ms     1s")
    print("   Cached prediction      < 1ms       5ms")
    print("   Model loading          < 1s        5s")
    print()
    
    print("ðŸ“Š MEASURING RESPONSE TIME:")
    print()
    print("   start_time = time.time()")
    print("   result = ml_service.predict(features)")
    print("   elapsed = time.time() - start_time")
    print("   ")
    print("   assert elapsed < 0.01  # 10ms")
    print()


# ============================================================================
# 4. Throughput Testing
# ============================================================================

def test_throughput_sequential(ml_service):
    """Test throughput with sequential requests."""
    features = [0.5] * 100
    n_requests = 1000
    
    start_time = time.time()
    for _ in range(n_requests):
        ml_service.predict(features)
    elapsed = time.time() - start_time
    
    throughput = n_requests / elapsed
    
    # Should handle at least 100 requests per second
    assert throughput >= 100, f"Low throughput: {throughput:.0f} req/s"
    
    print(f"Throughput: {throughput:.0f} requests/second")


def test_throughput_concurrent(ml_service):
    """Test throughput with concurrent requests."""
    features = [0.5] * 100
    n_requests = 1000
    n_workers = 10
    
    def make_request():
        return ml_service.predict(features)
    
    start_time = time.time()
    with concurrent.futures.ThreadPoolExecutor(max_workers=n_workers) as executor:
        futures = [executor.submit(make_request) for _ in range(n_requests)]
        results = [f.result() for f in futures]
    elapsed = time.time() - start_time
    
    throughput = n_requests / elapsed
    
    # Concurrent should be faster than sequential
    print(f"Concurrent throughput: {throughput:.0f} requests/second")
    
    # Should handle at least 500 requests per second with concurrency
    assert throughput >= 500, f"Low concurrent throughput: {throughput:.0f} req/s"


def demo_throughput_testing():
    """Demo throughput testing."""
    print("\n" + "=" * 70)
    print("3. Throughput Testing")
    print("=" * 70)
    print()
    
    print("ðŸ“ˆ THROUGHPUT METRICS:")
    print()
    print("   Sequential:")
    print("   â€¢ Single thread")
    print("   â€¢ Baseline performance")
    print("   â€¢ Target: > 100 req/s")
    print()
    print("   Concurrent:")
    print("   â€¢ Multiple threads/processes")
    print("   â€¢ Real-world load")
    print("   â€¢ Target: > 500 req/s")
    print()
    
    print("ðŸ’¡ THROUGHPUT FORMULA:")
    print()
    print("   throughput = n_requests / elapsed_time")
    print("   ")
    print("   Example:")
    print("   â€¢ 1000 requests in 2 seconds")
    print("   â€¢ throughput = 1000 / 2 = 500 req/s")
    print()


# ============================================================================
# 5. Memory Usage Testing
# ============================================================================

def test_memory_usage_single_prediction(ml_service):
    """Test memory usage for single prediction."""
    process = psutil.Process(os.getpid())
    
    # Measure before
    mem_before = process.memory_info().rss / 1024 / 1024  # MB
    
    # Make predictions
    features = [0.5] * 100
    for _ in range(1000):
        ml_service.predict(features)
    
    # Measure after
    mem_after = process.memory_info().rss / 1024 / 1024  # MB
    
    mem_increase = mem_after - mem_before
    
    # Memory shouldn't increase by more than 50MB
    assert mem_increase < 50, f"Memory leak: +{mem_increase:.1f}MB"
    
    print(f"Memory increase: {mem_increase:.1f}MB")


def test_memory_usage_batch_prediction(ml_service):
    """Test memory usage for batch prediction."""
    process = psutil.Process(os.getpid())
    
    # Measure before
    mem_before = process.memory_info().rss / 1024 / 1024  # MB
    
    # Make batch predictions
    batch_features = [[0.5] * 100 for _ in range(1000)]
    ml_service.predict_batch(batch_features)
    
    # Measure after
    mem_after = process.memory_info().rss / 1024 / 1024  # MB
    
    mem_increase = mem_after - mem_before
    
    # Memory shouldn't increase by more than 100MB
    assert mem_increase < 100, f"Memory leak: +{mem_increase:.1f}MB"
    
    print(f"Batch memory increase: {mem_increase:.1f}MB")


def demo_memory_testing():
    """Demo memory usage testing."""
    print("\n" + "=" * 70)
    print("4. Memory Usage Testing")
    print("=" * 70)
    print()
    
    print("ðŸ’¾ MEMORY TESTING:")
    print()
    print("   import psutil")
    print("   process = psutil.Process(os.getpid())")
    print("   ")
    print("   mem_before = process.memory_info().rss")
    print("   # run predictions...")
    print("   mem_after = process.memory_info().rss")
    print("   ")
    print("   mem_increase = mem_after - mem_before")
    print("   assert mem_increase < threshold")
    print()
    
    print("ðŸš¨ MEMORY LEAK SIGNS:")
    print()
    print("   â€¢ Memory keeps growing")
    print("   â€¢ Never gets released")
    print("   â€¢ Eventually crashes")
    print()
    print("   Common causes:")
    print("   â€¢ Not releasing tensors")
    print("   â€¢ Unbounded caches")
    print("   â€¢ Circular references")
    print("   â€¢ Not closing connections")
    print()


# ============================================================================
# 6. Stress Testing
# ============================================================================

@pytest.mark.slow
def test_stress_many_requests(ml_service):
    """Stress test with many requests."""
    features = [0.5] * 100
    n_requests = 10000
    
    start_time = time.time()
    
    # Make many requests
    for i in range(n_requests):
        try:
            ml_service.predict(features)
        except Exception as e:
            pytest.fail(f"Failed at request {i}: {e}")
    
    elapsed = time.time() - start_time
    throughput = n_requests / elapsed
    
    print(f"Stress test: {n_requests} requests in {elapsed:.1f}s ({throughput:.0f} req/s)")
    
    # Should complete without errors
    assert True


@pytest.mark.slow
def test_stress_large_batch(ml_service):
    """Stress test with large batch."""
    # Very large batch
    large_batch = [[0.5] * 100 for _ in range(10000)]
    
    start_time = time.time()
    results = ml_service.predict_batch(large_batch)
    elapsed = time.time() - start_time
    
    print(f"Large batch: 10000 predictions in {elapsed:.1f}s")
    
    # Should handle large batch
    assert len(results) == 10000
    assert elapsed < 30  # Should complete in reasonable time


def demo_stress_testing():
    """Demo stress testing."""
    print("\n" + "=" * 70)
    print("5. Stress Testing")
    print("=" * 70)
    print()
    
    print("ðŸ”¥ STRESS TEST GOALS:")
    print()
    print("   â€¢ Find breaking point")
    print("   â€¢ Test under extreme load")
    print("   â€¢ Verify graceful degradation")
    print("   â€¢ Identify resource limits")
    print()
    
    print("ðŸ“Š STRESS TEST SCENARIOS:")
    print()
    print("   1. Many Requests:")
    print("      â€¢ 10,000+ sequential requests")
    print("      â€¢ Should not crash")
    print("   ")
    print("   2. Large Batches:")
    print("      â€¢ 10,000+ items in one batch")
    print("      â€¢ Should handle gracefully")
    print("   ")
    print("   3. Concurrent Load:")
    print("      â€¢ 100+ concurrent users")
    print("      â€¢ Should maintain performance")
    print("   ")
    print("   4. Extended Duration:")
    print("      â€¢ Run for hours/days")
    print("      â€¢ Check for memory leaks")
    print()


# ============================================================================
# 7. Latency Percentiles
# ============================================================================

def test_latency_percentiles(ml_service):
    """Test latency percentiles (P50, P95, P99)."""
    features = [0.5] * 100
    n_requests = 1000
    latencies = []
    
    # Measure latency for each request
    for _ in range(n_requests):
        start_time = time.time()
        ml_service.predict(features)
        elapsed = (time.time() - start_time) * 1000  # Convert to ms
        latencies.append(elapsed)
    
    # Calculate percentiles
    p50 = np.percentile(latencies, 50)
    p95 = np.percentile(latencies, 95)
    p99 = np.percentile(latencies, 99)
    
    print(f"Latency percentiles:")
    print(f"  P50: {p50:.2f}ms")
    print(f"  P95: {p95:.2f}ms")
    print(f"  P99: {p99:.2f}ms")
    
    # Targets
    assert p50 < 10, f"P50 too high: {p50:.2f}ms"
    assert p95 < 50, f"P95 too high: {p95:.2f}ms"
    assert p99 < 100, f"P99 too high: {p99:.2f}ms"


def demo_latency_percentiles():
    """Demo latency percentiles."""
    print("\n" + "=" * 70)
    print("6. Latency Percentiles")
    print("=" * 70)
    print()
    
    print("ðŸ“Š UNDERSTANDING PERCENTILES:")
    print()
    print("   P50 (median):")
    print("   â€¢ 50% of requests faster")
    print("   â€¢ Typical user experience")
    print()
    print("   P95:")
    print("   â€¢ 95% of requests faster")
    print("   â€¢ Most users' worst case")
    print()
    print("   P99:")
    print("   â€¢ 99% of requests faster")
    print("   â€¢ Worst case scenario")
    print()
    
    print("ðŸŽ¯ LATENCY TARGETS:")
    print()
    print("   ML API:")
    print("   â€¢ P50: < 10ms")
    print("   â€¢ P95: < 50ms")
    print("   â€¢ P99: < 100ms")
    print()
    print("   Why P99 matters:")
    print("   â€¢ 1000 requests/sec")
    print("   â€¢ 1% = 10 requests/sec with bad latency")
    print("   â€¢ = 36,000 bad experiences per hour!")
    print()


# ============================================================================
# Run Demonstrations
# ============================================================================

def main():
    """Run all demonstrations."""
    print("\nðŸ§ª Performance and Load Testing\n")
    
    demo_why_performance_testing()
    demo_response_time_testing()
    demo_throughput_testing()
    demo_memory_testing()
    demo_stress_testing()
    demo_latency_percentiles()
    
    print("\n" + "=" * 70)
    print("âœ… Key Takeaways")
    print("=" * 70)
    print("""
1. Why Performance Testing:
   - Prevent production disasters
   - Meet user expectations (< 1s)
   - Optimize costs (right-size infrastructure)
   - ROI: Prevent $50k+ outages

2. Response Time:
   âœ… Single prediction < 10ms
   âœ… Batch (100) < 100ms
   âœ… Cached < 1ms
   âœ… Model loading < 1s

3. Throughput:
   âœ… Sequential: > 100 req/s
   âœ… Concurrent: > 500 req/s
   âœ… Test with ThreadPoolExecutor

4. Memory:
   âœ… Monitor with psutil
   âœ… Check for memory leaks
   âœ… Memory increase < 50MB for 1000 predictions

5. Stress Testing:
   âœ… 10,000+ requests
   âœ… Large batches (10,000 items)
   âœ… Concurrent users (100+)
   âœ… Extended duration (hours)

6. Latency Percentiles:
   âœ… P50 < 10ms
   âœ… P95 < 50ms
   âœ… P99 < 100ms

Performance Testing Checklist:
```
Response Time:
â–¡ Single prediction < 10ms
â–¡ Batch prediction < 100ms
â–¡ Cached predictions faster
â–¡ Scales with input size

Throughput:
â–¡ Sequential > 100 req/s
â–¡ Concurrent > 500 req/s
â–¡ Measure actual throughput

Memory:
â–¡ No memory leaks
â–¡ Memory increase reasonable
â–¡ Resources released

Stress Tests:
â–¡ Handle 10,000+ requests
â–¡ Handle large batches
â–¡ Handle concurrent load
â–¡ No crashes under stress

Latency:
â–¡ P50 < 10ms
â–¡ P95 < 50ms
â–¡ P99 < 100ms
```

Performance Testing Tools:
```python
# Response time
start = time.time()
result = service.predict(features)
elapsed = time.time() - start
assert elapsed < 0.01  # 10ms

# Throughput
throughput = n_requests / elapsed

# Memory
import psutil
process = psutil.Process(os.getpid())
mem = process.memory_info().rss / 1024 / 1024  # MB

# Latency percentiles
import numpy as np
p95 = np.percentile(latencies, 95)

# Concurrent testing
from concurrent.futures import ThreadPoolExecutor
with ThreadPoolExecutor(max_workers=10) as executor:
    futures = [executor.submit(predict) for _ in range(1000)]
```

Performance Targets:
```
User Experience:
< 100ms: Instant
< 1s: Fast  
< 3s: Acceptable
> 3s: Slow (users leave)

Business Impact:
Amazon: 100ms delay = 1% sales loss
Google: 500ms delay = 20% traffic loss
Your API: Slow = Users leave
```

Next Steps:
âœ… Module 11 Complete!
â†’ Module 12: Complete Production Projects
""")


if __name__ == "__main__":
    main()
