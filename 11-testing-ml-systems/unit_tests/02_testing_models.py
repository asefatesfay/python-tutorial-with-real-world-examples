"""
Testing ML Models with pytest

Learn how to test machine learning models effectively.
Focus: Model predictions, performance, edge cases.

Install: poetry add --group dev pytest scikit-learn numpy
Run: pytest unit_tests/02_testing_models.py -v
"""

import pytest
import numpy as np
from typing import List, Tuple
from dataclasses import dataclass


# ============================================================================
# 1. Why Testing ML Models is Different
# ============================================================================

def demo_why_ml_testing_is_different():
    """
    ML testing is different from traditional software testing.
    """
    print("=" * 70)
    print("1. Why ML Testing is Different")
    print("=" * 70)
    print()
    
    print("ğŸ¤” TRADITIONAL SOFTWARE:")
    print()
    print("   def add(a, b):")
    print("       return a + b")
    print("   ")
    print("   # Test:")
    print("   assert add(2, 3) == 5  # Always 5! Deterministic âœ…")
    print()
    
    print("ğŸ§  MACHINE LEARNING:")
    print()
    print("   model.predict([[2, 3]])")
    print("   # Output: 0.847  â† Not deterministic!")
    print("   # ")
    print("   # Questions:")
    print("   # â€¢ Is 0.847 correct? (No ground truth)")
    print("   # â€¢ Should it be exactly 0.847? (Random seeds)")
    print("   # â€¢ What if it's 0.846? (Close enough?)")
    print()
    
    print("ğŸ¯ KEY DIFFERENCES:")
    print()
    print("   Traditional Testing           ML Testing")
    print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print("   âœ“ Deterministic               âœ— Probabilistic")
    print("   âœ“ Exact answers               âœ— Approximate")
    print("   âœ“ Fast execution              âœ— Can be slow")
    print("   âœ“ Clear correctness           âœ— Subjective")
    print("   âœ“ No randomness               âœ— Random seeds matter")
    print()
    
    print("ğŸ’¡ WHAT TO TEST IN ML:")
    print()
    print("   Instead of 'Is prediction correct?':")
    print()
    print("   âœ… 1. Output Shape:")
    print("      predict([[1, 2]]) â†’ shape (1,)")
    print("   ")
    print("   âœ… 2. Output Type:")
    print("      predict(...) â†’ numpy array, not list")
    print("   ")
    print("   âœ… 3. Output Range:")
    print("      predict(...) â†’ values between 0 and 1")
    print("   ")
    print("   âœ… 4. Edge Cases:")
    print("      predict([]) â†’ raises ValueError")
    print("      predict(None) â†’ raises TypeError")
    print("   ")
    print("   âœ… 5. Invariants:")
    print("      predict(X) twice â†’ same result (reproducibility)")
    print("      predict([X1, X2]) == [predict(X1), predict(X2)]")
    print("   ")
    print("   âœ… 6. Performance:")
    print("      predict(1000 samples) < 1 second")
    print()


# ============================================================================
# 2. Simple Model to Test
# ============================================================================

class SimpleClassifier:
    """
    Simple ML classifier for testing examples.
    
    Predicts class based on simple threshold rule.
    """
    
    def __init__(self, threshold: float = 0.5):
        self.threshold = threshold
        self.is_trained = False
        self.n_features = None
    
    def fit(self, X: List[List[float]], y: List[int]):
        """Train the model."""
        self.is_trained = True
        self.n_features = len(X[0]) if X else 0
        return self
    
    def predict(self, X: List[List[float]]) -> np.ndarray:
        """Predict classes."""
        if not self.is_trained:
            raise ValueError("Model not trained. Call fit() first.")
        
        if not X:
            raise ValueError("Input X cannot be empty")
        
        # Check feature count
        for sample in X:
            if len(sample) != self.n_features:
                raise ValueError(
                    f"Expected {self.n_features} features, got {len(sample)}"
                )
        
        # Simple rule: predict 1 if average > threshold
        predictions = []
        for sample in X:
            avg = sum(sample) / len(sample)
            predictions.append(1 if avg > self.threshold else 0)
        
        return np.array(predictions)
    
    def predict_proba(self, X: List[List[float]]) -> np.ndarray:
        """Predict probabilities."""
        if not self.is_trained:
            raise ValueError("Model not trained. Call fit() first.")
        
        probas = []
        for sample in X:
            avg = sum(sample) / len(sample)
            # Convert to probability (simple sigmoid-like)
            prob_class_1 = avg / (1 + avg)
            probas.append([1 - prob_class_1, prob_class_1])
        
        return np.array(probas)


# ============================================================================
# 3. Testing Model Predictions
# ============================================================================

@pytest.fixture
def trained_model():
    """Fixture providing a trained model."""
    model = SimpleClassifier(threshold=0.5)
    X_train = [[0.1, 0.2], [0.8, 0.9], [0.3, 0.4]]
    y_train = [0, 1, 0]
    model.fit(X_train, y_train)
    return model


def test_model_is_trained(trained_model):
    """Test that model is marked as trained."""
    assert trained_model.is_trained is True


def test_model_predicts_correct_shape(trained_model):
    """Test prediction output shape."""
    X_test = [[0.6, 0.7], [0.2, 0.3]]
    predictions = trained_model.predict(X_test)
    
    # Check shape
    assert predictions.shape == (2,)
    assert len(predictions) == 2


def test_model_predicts_correct_type(trained_model):
    """Test prediction output type."""
    X_test = [[0.6, 0.7]]
    predictions = trained_model.predict(X_test)
    
    # Check type
    assert isinstance(predictions, np.ndarray)
    assert predictions.dtype in [np.int32, np.int64]


def test_model_predicts_correct_range(trained_model):
    """Test prediction values are in valid range."""
    X_test = [[0.6, 0.7], [0.2, 0.3], [0.9, 0.8]]
    predictions = trained_model.predict(X_test)
    
    # Binary classification: only 0 or 1
    assert all(p in [0, 1] for p in predictions)


def test_model_predict_proba_shape(trained_model):
    """Test probability prediction shape."""
    X_test = [[0.6, 0.7], [0.2, 0.3]]
    probas = trained_model.predict_proba(X_test)
    
    # Shape: (n_samples, n_classes)
    assert probas.shape == (2, 2)


def test_model_predict_proba_sum_to_one(trained_model):
    """Test probabilities sum to 1."""
    X_test = [[0.6, 0.7], [0.2, 0.3]]
    probas = trained_model.predict_proba(X_test)
    
    # Each row sums to 1
    for proba_row in probas:
        assert pytest.approx(sum(proba_row), abs=0.01) == 1.0


def test_model_predict_proba_range(trained_model):
    """Test probabilities are between 0 and 1."""
    X_test = [[0.6, 0.7], [0.2, 0.3]]
    probas = trained_model.predict_proba(X_test)
    
    # All probabilities between 0 and 1
    assert np.all(probas >= 0)
    assert np.all(probas <= 1)


def demo_testing_predictions():
    """Demo testing model predictions."""
    print("\n" + "=" * 70)
    print("2. Testing Model Predictions")
    print("=" * 70)
    print()
    
    print("âœ… WHAT TO TEST:")
    print()
    print("   1. Output Shape:")
    print("      predictions.shape == (n_samples,)")
    print("   ")
    print("   2. Output Type:")
    print("      isinstance(predictions, np.ndarray)")
    print("   ")
    print("   3. Output Range:")
    print("      all(p in [0, 1] for p in predictions)")
    print("   ")
    print("   4. Probabilities:")
    print("      sum(proba) â‰ˆ 1.0  # Each row sums to 1")
    print("      0 â‰¤ proba â‰¤ 1     # Valid range")
    print()


# ============================================================================
# 4. Testing Edge Cases
# ============================================================================

def test_model_raises_error_when_not_trained():
    """Test that untrained model raises error."""
    model = SimpleClassifier()
    
    with pytest.raises(ValueError, match="Model not trained"):
        model.predict([[0.5, 0.6]])


def test_model_raises_error_on_empty_input(trained_model):
    """Test that empty input raises error."""
    with pytest.raises(ValueError, match="cannot be empty"):
        trained_model.predict([])


def test_model_raises_error_on_wrong_feature_count(trained_model):
    """Test that wrong number of features raises error."""
    # Model trained with 2 features
    X_wrong = [[0.5, 0.6, 0.7]]  # 3 features!
    
    with pytest.raises(ValueError, match="Expected 2 features, got 3"):
        trained_model.predict(X_wrong)


def test_model_handles_single_sample(trained_model):
    """Test model with single sample."""
    X_single = [[0.6, 0.7]]
    predictions = trained_model.predict(X_single)
    
    assert predictions.shape == (1,)
    assert predictions[0] in [0, 1]


def test_model_handles_large_batch(trained_model):
    """Test model with large batch."""
    # Generate 1000 samples
    X_large = [[0.5, 0.6] for _ in range(1000)]
    predictions = trained_model.predict(X_large)
    
    assert predictions.shape == (1000,)


def demo_testing_edge_cases():
    """Demo testing edge cases."""
    print("\n" + "=" * 70)
    print("3. Testing Edge Cases")
    print("=" * 70)
    print()
    
    print("ğŸš¨ EDGE CASES TO TEST:")
    print()
    print("   1. Not Trained:")
    print("      model.predict(...) before fit()")
    print("      â†’ ValueError âœ…")
    print("   ")
    print("   2. Empty Input:")
    print("      model.predict([])")
    print("      â†’ ValueError âœ…")
    print("   ")
    print("   3. Wrong Features:")
    print("      Trained: 2 features, Input: 3 features")
    print("      â†’ ValueError âœ…")
    print("   ")
    print("   4. Single Sample:")
    print("      model.predict([[0.5, 0.6]])")
    print("      â†’ Works correctly âœ…")
    print("   ")
    print("   5. Large Batch:")
    print("      model.predict([[...]  * 1000)")
    print("      â†’ Handles efficiently âœ…")
    print()
    
    print("ğŸ’¡ WHY EDGE CASES MATTER:")
    print()
    print("   Production scenarios:")
    print("   â€¢ User uploads empty file â†’ Empty input")
    print("   â€¢ Data schema changes â†’ Wrong features")
    print("   â€¢ High traffic â†’ Large batches")
    print("   â€¢ Service restart â†’ Model not loaded")
    print()
    print("   Without tests:")
    print("   â€¢ App crashes in production")
    print("   â€¢ Poor user experience")
    print("   â€¢ Debugging takes hours")
    print()
    print("   With tests:")
    print("   â€¢ Catch before deployment âœ…")
    print("   â€¢ Graceful error handling âœ…")
    print("   â€¢ Fast debugging âœ…")
    print()


# ============================================================================
# 5. Testing Model Invariants
# ============================================================================

def test_model_reproducibility(trained_model):
    """Test that predictions are reproducible."""
    X_test = [[0.6, 0.7], [0.2, 0.3]]
    
    # Predict twice
    predictions_1 = trained_model.predict(X_test)
    predictions_2 = trained_model.predict(X_test)
    
    # Should be identical
    assert np.array_equal(predictions_1, predictions_2)


def test_model_prediction_consistency(trained_model):
    """Test that batch prediction matches individual predictions."""
    X_test = [[0.6, 0.7], [0.2, 0.3]]
    
    # Batch prediction
    batch_predictions = trained_model.predict(X_test)
    
    # Individual predictions
    individual_predictions = [
        trained_model.predict([X_test[0]])[0],
        trained_model.predict([X_test[1]])[0],
    ]
    
    # Should match
    assert np.array_equal(batch_predictions, individual_predictions)


def test_model_prediction_order_independence(trained_model):
    """Test that prediction order doesn't matter."""
    X_test = [[0.6, 0.7], [0.2, 0.3]]
    X_test_reversed = [[0.2, 0.3], [0.6, 0.7]]
    
    predictions = trained_model.predict(X_test)
    predictions_reversed = trained_model.predict(X_test_reversed)
    
    # Reversed predictions should match reversed order
    assert predictions[0] == predictions_reversed[1]
    assert predictions[1] == predictions_reversed[0]


def demo_testing_invariants():
    """Demo testing model invariants."""
    print("\n" + "=" * 70)
    print("4. Testing Model Invariants")
    print("=" * 70)
    print()
    
    print("ğŸ” WHAT ARE INVARIANTS?")
    print()
    print("   Properties that should ALWAYS hold true.")
    print()
    print("   Examples:")
    print("   â€¢ Same input â†’ Same output (reproducibility)")
    print("   â€¢ Batch prediction = Individual predictions (consistency)")
    print("   â€¢ Order doesn't matter (unless it should!)")
    print()
    
    print("âœ… TESTING INVARIANTS:")
    print()
    print("   1. Reproducibility:")
    print("      predict(X) == predict(X)  # Always!")
    print("   ")
    print("   2. Consistency:")
    print("      predict([X1, X2]) == [predict(X1), predict(X2)]")
    print("   ")
    print("   3. Order Independence:")
    print("      predict([X1, X2]) â‰  predict([X2, X1])  # Order matters")
    print("      But: predict([X1, X2])[0] == predict([X2, X1])[1]")
    print()
    
    print("ğŸ’¡ WHY INVARIANTS MATTER:")
    print()
    print("   Bug example:")
    print("   â€¢ User gets prediction: 0.85")
    print("   â€¢ Refreshes page: 0.91  â† Different!")
    print("   â€¢ User confused: 'Which is correct?'")
    print()
    print("   With invariant tests:")
    print("   â€¢ Catch non-reproducibility early")
    print("   â€¢ Ensure consistent user experience")
    print("   â€¢ Build trust in ML system")
    print()


# ============================================================================
# 6. Testing Model Performance
# ============================================================================

import time

def test_model_prediction_speed(trained_model):
    """Test that predictions are fast enough."""
    # Generate 1000 samples
    X_test = [[0.5, 0.6] for _ in range(1000)]
    
    start_time = time.time()
    trained_model.predict(X_test)
    end_time = time.time()
    
    elapsed = end_time - start_time
    
    # Should take less than 1 second
    assert elapsed < 1.0, f"Prediction too slow: {elapsed:.2f}s"


def test_model_memory_efficiency(trained_model):
    """Test that model doesn't consume excessive memory."""
    import sys
    
    X_test = [[0.5, 0.6] for _ in range(1000)]
    
    # Predict
    predictions = trained_model.predict(X_test)
    
    # Check memory size
    prediction_size = sys.getsizeof(predictions)
    
    # Should be reasonable (< 1MB for 1000 predictions)
    assert prediction_size < 1_000_000


def demo_testing_performance():
    """Demo testing model performance."""
    print("\n" + "=" * 70)
    print("5. Testing Model Performance")
    print("=" * 70)
    print()
    
    print("âš¡ PERFORMANCE TESTS:")
    print()
    print("   1. Prediction Speed:")
    print("      â€¢ 1000 predictions < 1 second")
    print("      â€¢ API response < 100ms")
    print("   ")
    print("   2. Memory Usage:")
    print("      â€¢ Model size < 100MB")
    print("      â€¢ Prediction memory < 1GB")
    print("   ")
    print("   3. Throughput:")
    print("      â€¢ Handle 100 requests/second")
    print("   ")
    print("   4. Latency:")
    print("      â€¢ P95 latency < 200ms")
    print("      â€¢ P99 latency < 500ms")
    print()
    
    print("ğŸ’¡ WHY PERFORMANCE MATTERS:")
    print()
    print("   Real-world constraints:")
    print("   â€¢ API timeout: 30 seconds")
    print("   â€¢ User patience: 2 seconds")
    print("   â€¢ Server memory: 8GB")
    print("   â€¢ Cost: $0.10 per 1000 predictions")
    print()
    print("   Slow model:")
    print("   â€¢ Users leave (high bounce rate)")
    print("   â€¢ High cloud costs")
    print("   â€¢ Poor scalability")
    print()
    print("   Fast model:")
    print("   â€¢ Happy users âœ…")
    print("   â€¢ Low costs âœ…")
    print("   â€¢ Scales well âœ…")
    print()


# ============================================================================
# 7. Parametrized Model Tests
# ============================================================================

@pytest.mark.parametrize("threshold", [0.3, 0.5, 0.7])
def test_model_with_different_thresholds(threshold):
    """Test model with different threshold values."""
    model = SimpleClassifier(threshold=threshold)
    X_train = [[0.1, 0.2], [0.8, 0.9]]
    y_train = [0, 1]
    model.fit(X_train, y_train)
    
    # Predictions should work
    X_test = [[0.6, 0.7]]
    predictions = model.predict(X_test)
    
    assert predictions.shape == (1,)
    assert predictions[0] in [0, 1]


@pytest.mark.parametrize("n_samples", [1, 10, 100, 1000])
def test_model_with_different_batch_sizes(trained_model, n_samples):
    """Test model with different batch sizes."""
    X_test = [[0.5, 0.6] for _ in range(n_samples)]
    predictions = trained_model.predict(X_test)
    
    assert predictions.shape == (n_samples,)


def demo_parametrized_model_tests():
    """Demo parametrized model tests."""
    print("\n" + "=" * 70)
    print("6. Parametrized Model Tests")
    print("=" * 70)
    print()
    
    print("ğŸ” PARAMETRIZE MODEL TESTS:")
    print()
    print("   @pytest.mark.parametrize('threshold', [0.3, 0.5, 0.7])")
    print("   def test_model_with_thresholds(threshold):")
    print("       model = SimpleClassifier(threshold=threshold)")
    print("       # Test with different thresholds")
    print("   ")
    print("   Result: Tests model with 3 different configurations!")
    print()
    
    print("ğŸ’¡ WHEN TO PARAMETRIZE:")
    print()
    print("   â€¢ Different hyperparameters")
    print("   â€¢ Different input sizes")
    print("   â€¢ Different data types")
    print("   â€¢ Different edge cases")
    print()


# ============================================================================
# Run Demonstrations
# ============================================================================

def main():
    """Run all demonstrations."""
    print("\nğŸ§ª Testing ML Models\n")
    
    demo_why_ml_testing_is_different()
    demo_testing_predictions()
    demo_testing_edge_cases()
    demo_testing_invariants()
    demo_testing_performance()
    demo_parametrized_model_tests()
    
    print("\n" + "=" * 70)
    print("âœ… Key Takeaways")
    print("=" * 70)
    print("""
1. ML Testing is Different:
   - No exact "correct" answers
   - Test shape, type, range instead
   - Test invariants and properties

2. What to Test:
   âœ… Output shape: predictions.shape == (n,)
   âœ… Output type: isinstance(predictions, np.ndarray)
   âœ… Output range: 0 â‰¤ predictions â‰¤ 1
   âœ… Reproducibility: predict(X) == predict(X)
   âœ… Edge cases: empty input, wrong features
   âœ… Performance: speed, memory

3. Edge Cases:
   - Model not trained
   - Empty input
   - Wrong feature count
   - Single sample
   - Large batch

4. Invariants:
   - Same input â†’ Same output
   - Batch = Individual predictions
   - Order independence (usually)

5. Performance:
   - Prediction speed < 1s for 1000 samples
   - Memory usage reasonable
   - Handle production load

Testing Checklist:
```
Model Tests:
â–¡ Output shape correct
â–¡ Output type correct
â–¡ Output range valid
â–¡ Probabilities sum to 1
â–¡ Handles not trained error
â–¡ Handles empty input error
â–¡ Handles wrong features error
â–¡ Handles single sample
â–¡ Handles large batch
â–¡ Reproducible predictions
â–¡ Consistent batch vs individual
â–¡ Fast enough (<1s for 1000)
â–¡ Memory efficient
```

Next Steps:
â†’ 03_testing_pipelines.py (Test data pipelines)
â†’ 04_testing_apis.py (Test ML APIs)
""")


if __name__ == "__main__":
    main()
