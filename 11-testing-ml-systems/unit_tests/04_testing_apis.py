"""
Testing ML APIs with pytest

Learn how to test FastAPI endpoints for ML services.
Focus: API endpoints, request/response, error handling, performance.

Install: poetry add --group dev pytest pytest-asyncio fastapi httpx
Run: pytest unit_tests/04_testing_apis.py -v
"""

import pytest
from fastapi import FastAPI, HTTPException
from fastapi.testclient import TestClient
from pydantic import BaseModel, Field
from typing import List, Optional
import time


# ============================================================================
# 1. Why Test ML APIs?
# ============================================================================

def demo_why_test_apis():
    """
    Why testing ML APIs is critical.
    """
    print("=" * 70)
    print("1. Why Test ML APIs?")
    print("=" * 70)
    print()
    
    print("ðŸ’¥ REAL-WORLD API HORROR STORIES:")
    print()
    print("   Story 1: The 500 Error")
    print("   â€¢ Endpoint: /predict")
    print("   â€¢ Bug: Missing validation on input")
    print("   â€¢ User sends: {'age': 'twenty'}")
    print("   â€¢ Response: 500 Internal Server Error")
    print("   â€¢ Impact: Users can't use API ðŸ’€")
    print("   â€¢ Cost: Lost revenue + support tickets")
    print()
    print("   Story 2: The Timeout")
    print("   â€¢ Endpoint: /predict")
    print("   â€¢ Bug: Model loading takes 30 seconds")
    print("   â€¢ Result: Every request times out")
    print("   â€¢ Impact: 100% failure rate ðŸ’€")
    print("   â€¢ Cost: Complete service outage")
    print()
    print("   Story 3: The Memory Leak")
    print("   â€¢ Endpoint: /batch_predict")
    print("   â€¢ Bug: Not releasing memory after prediction")
    print("   â€¢ Result: Server crashes after 1000 requests")
    print("   â€¢ Impact: Need frequent restarts ðŸ’€")
    print("   â€¢ Cost: Poor reliability")
    print()
    
    print("ðŸŽ¯ WHY API TESTS MATTER:")
    print()
    print("   APIs are user-facing:")
    print("   â€¢ Bad API = Bad user experience")
    print("   â€¢ Errors = Support tickets")
    print("   â€¢ Slow = Users leave")
    print("   â€¢ Crashes = Lost revenue")
    print()
    print("   Tests prevent:")
    print("   â€¢ 500 errors (validation)")
    print("   â€¢ Timeouts (performance)")
    print("   â€¢ Memory leaks (load testing)")
    print("   â€¢ Security issues (input sanitization)")
    print()
    
    print("âœ… WHAT TO TEST:")
    print()
    print("   1. Happy Path:")
    print("      â€¢ Valid input â†’ 200 OK")
    print("      â€¢ Correct response format")
    print("   ")
    print("   2. Error Cases:")
    print("      â€¢ Invalid input â†’ 422 Validation Error")
    print("      â€¢ Missing fields â†’ 422")
    print("      â€¢ Server error â†’ 500")
    print("   ")
    print("   3. Performance:")
    print("      â€¢ Response time < 1s")
    print("      â€¢ Handle 100 requests/sec")
    print("   ")
    print("   4. Security:")
    print("      â€¢ SQL injection prevented")
    print("      â€¢ XSS prevented")
    print("      â€¢ Rate limiting works")
    print()


# ============================================================================
# 2. Simple ML API
# ============================================================================

# Pydantic models for request/response
class PredictionRequest(BaseModel):
    """Request model for prediction endpoint."""
    features: List[float] = Field(..., min_items=1, max_items=100)
    model_version: Optional[str] = Field(default="v1")
    
    class Config:
        json_schema_extra = {
            "example": {
                "features": [0.5, 0.6, 0.7],
                "model_version": "v1"
            }
        }


class PredictionResponse(BaseModel):
    """Response model for prediction endpoint."""
    prediction: float
    confidence: float = Field(..., ge=0, le=1)
    model_version: str
    latency_ms: float


class HealthResponse(BaseModel):
    """Response model for health check."""
    status: str
    model_loaded: bool
    version: str


# Simple ML model (mock)
class SimpleModel:
    """Simple model for testing."""
    
    def __init__(self):
        self.is_loaded = True
        self.version = "v1"
    
    def predict(self, features: List[float]) -> float:
        """Make prediction."""
        if not features:
            raise ValueError("Features cannot be empty")
        
        # Simple rule: average of features
        return sum(features) / len(features)
    
    def predict_with_confidence(self, features: List[float]) -> tuple[float, float]:
        """Make prediction with confidence."""
        prediction = self.predict(features)
        
        # Simple confidence: inverse of variance
        variance = sum((f - prediction) ** 2 for f in features) / len(features)
        confidence = 1.0 / (1.0 + variance)
        
        return prediction, confidence


# FastAPI app
app = FastAPI(title="ML API", version="1.0.0")
model = SimpleModel()


@app.get("/health", response_model=HealthResponse)
def health_check():
    """Health check endpoint."""
    return HealthResponse(
        status="healthy",
        model_loaded=model.is_loaded,
        version=model.version
    )


@app.post("/predict", response_model=PredictionResponse)
def predict(request: PredictionRequest):
    """Prediction endpoint."""
    start_time = time.time()
    
    try:
        prediction, confidence = model.predict_with_confidence(request.features)
        latency_ms = (time.time() - start_time) * 1000
        
        return PredictionResponse(
            prediction=prediction,
            confidence=confidence,
            model_version=request.model_version or "v1",
            latency_ms=latency_ms
        )
    except ValueError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail="Internal server error")


# ============================================================================
# 3. Testing API Endpoints
# ============================================================================

@pytest.fixture
def client():
    """Fixture providing test client."""
    return TestClient(app)


def test_health_check_returns_200(client):
    """Test that health check returns 200 OK."""
    response = client.get("/health")
    assert response.status_code == 200


def test_health_check_response_format(client):
    """Test health check response format."""
    response = client.get("/health")
    data = response.json()
    
    # Check fields exist
    assert "status" in data
    assert "model_loaded" in data
    assert "version" in data
    
    # Check values
    assert data["status"] == "healthy"
    assert data["model_loaded"] is True
    assert isinstance(data["version"], str)


def test_predict_valid_input(client):
    """Test prediction with valid input."""
    response = client.post("/predict", json={
        "features": [0.5, 0.6, 0.7],
        "model_version": "v1"
    })
    
    assert response.status_code == 200
    
    data = response.json()
    assert "prediction" in data
    assert "confidence" in data
    assert "model_version" in data
    assert "latency_ms" in data


def test_predict_response_types(client):
    """Test prediction response types."""
    response = client.post("/predict", json={
        "features": [0.5, 0.6, 0.7]
    })
    
    data = response.json()
    
    # Check types
    assert isinstance(data["prediction"], (int, float))
    assert isinstance(data["confidence"], (int, float))
    assert isinstance(data["model_version"], str)
    assert isinstance(data["latency_ms"], (int, float))


def test_predict_confidence_range(client):
    """Test that confidence is in valid range."""
    response = client.post("/predict", json={
        "features": [0.5, 0.6, 0.7]
    })
    
    data = response.json()
    
    # Confidence between 0 and 1
    assert 0 <= data["confidence"] <= 1


def demo_testing_endpoints():
    """Demo testing API endpoints."""
    print("\n" + "=" * 70)
    print("2. Testing API Endpoints")
    print("=" * 70)
    print()
    
    print("âœ… ENDPOINT TESTS:")
    print()
    print("   1. Status Code:")
    print("      assert response.status_code == 200")
    print("   ")
    print("   2. Response Format:")
    print("      assert 'prediction' in response.json()")
    print("   ")
    print("   3. Response Types:")
    print("      assert isinstance(data['prediction'], float)")
    print("   ")
    print("   4. Response Range:")
    print("      assert 0 <= data['confidence'] <= 1")
    print()
    
    print("ðŸ”§ TEST CLIENT:")
    print()
    print("   from fastapi.testclient import TestClient")
    print("   ")
    print("   client = TestClient(app)")
    print("   response = client.get('/health')")
    print("   assert response.status_code == 200")
    print()


# ============================================================================
# 4. Testing Error Handling
# ============================================================================

def test_predict_missing_features(client):
    """Test prediction with missing features."""
    response = client.post("/predict", json={
        "model_version": "v1"
    })
    
    # Should return 422 Validation Error
    assert response.status_code == 422


def test_predict_empty_features(client):
    """Test prediction with empty features."""
    response = client.post("/predict", json={
        "features": []
    })
    
    # Should return 422 (validation error)
    assert response.status_code == 422


def test_predict_invalid_feature_type(client):
    """Test prediction with invalid feature type."""
    response = client.post("/predict", json={
        "features": ["not", "a", "number"]
    })
    
    # Should return 422 (validation error)
    assert response.status_code == 422


def test_predict_too_many_features(client):
    """Test prediction with too many features."""
    response = client.post("/predict", json={
        "features": [0.5] * 101  # Max is 100
    })
    
    # Should return 422 (validation error)
    assert response.status_code == 422


def test_invalid_endpoint(client):
    """Test invalid endpoint returns 404."""
    response = client.get("/invalid")
    assert response.status_code == 404


def demo_testing_errors():
    """Demo testing error handling."""
    print("\n" + "=" * 70)
    print("3. Testing Error Handling")
    print("=" * 70)
    print()
    
    print("ðŸš¨ ERROR CASES TO TEST:")
    print()
    print("   1. Missing Required Fields:")
    print("      POST /predict without 'features'")
    print("      â†’ 422 Validation Error âœ…")
    print("   ")
    print("   2. Invalid Types:")
    print("      features: ['a', 'b', 'c']")
    print("      â†’ 422 Validation Error âœ…")
    print("   ")
    print("   3. Out of Range:")
    print("      features: [0.5] * 101  (max 100)")
    print("      â†’ 422 Validation Error âœ…")
    print("   ")
    print("   4. Invalid Endpoint:")
    print("      GET /invalid")
    print("      â†’ 404 Not Found âœ…")
    print()
    
    print("ðŸ’¡ WHY ERROR TESTS MATTER:")
    print()
    print("   Without validation:")
    print("   â€¢ Server crashes (500 error)")
    print("   â€¢ Poor user experience")
    print("   â€¢ Security vulnerabilities")
    print()
    print("   With validation:")
    print("   â€¢ Clear error messages (422)")
    print("   â€¢ Graceful failures")
    print("   â€¢ Better UX âœ…")
    print()


# ============================================================================
# 5. Testing Request Validation
# ============================================================================

@pytest.mark.parametrize("features,expected_status", [
    ([0.5, 0.6], 200),  # Valid
    ([0.1], 200),  # Valid (single feature)
    ([], 422),  # Empty
    ([0.5] * 100, 200),  # Max valid
    ([0.5] * 101, 422),  # Too many
])
def test_predict_feature_count_validation(client, features, expected_status):
    """Test feature count validation."""
    response = client.post("/predict", json={"features": features})
    assert response.status_code == expected_status


def test_predict_optional_model_version(client):
    """Test that model_version is optional."""
    # Without model_version
    response = client.post("/predict", json={
        "features": [0.5, 0.6]
    })
    
    assert response.status_code == 200
    data = response.json()
    assert "model_version" in data


def demo_testing_validation():
    """Demo testing request validation."""
    print("\n" + "=" * 70)
    print("4. Testing Request Validation")
    print("=" * 70)
    print()
    
    print("âœ… VALIDATION TESTS:")
    print()
    print("   Pydantic validation:")
    print("   class PredictionRequest(BaseModel):")
    print("       features: List[float] = Field(..., min_items=1, max_items=100)")
    print("       model_version: Optional[str] = Field(default='v1')")
    print()
    print("   Tests:")
    print("   â€¢ min_items=1: Empty list rejected")
    print("   â€¢ max_items=100: 101 items rejected")
    print("   â€¢ List[float]: Strings rejected")
    print("   â€¢ Optional[str]: Can omit model_version")
    print()


# ============================================================================
# 6. Testing API Performance
# ============================================================================

def test_predict_response_time(client):
    """Test that prediction is fast enough."""
    response = client.post("/predict", json={
        "features": [0.5, 0.6, 0.7]
    })
    
    data = response.json()
    latency_ms = data["latency_ms"]
    
    # Should be less than 100ms
    assert latency_ms < 100, f"Prediction too slow: {latency_ms}ms"


def test_predict_batch_performance(client):
    """Test performance with multiple requests."""
    start_time = time.time()
    
    # Send 100 requests
    for _ in range(100):
        response = client.post("/predict", json={
            "features": [0.5, 0.6, 0.7]
        })
        assert response.status_code == 200
    
    elapsed = time.time() - start_time
    
    # 100 requests in less than 2 seconds
    assert elapsed < 2.0, f"Batch too slow: {elapsed:.2f}s"


def test_health_check_response_time(client):
    """Test that health check is fast."""
    start_time = time.time()
    response = client.get("/health")
    elapsed = (time.time() - start_time) * 1000
    
    # Health check should be < 10ms
    assert elapsed < 10, f"Health check too slow: {elapsed:.2f}ms"


def demo_testing_performance():
    """Demo testing API performance."""
    print("\n" + "=" * 70)
    print("5. Testing API Performance")
    print("=" * 70)
    print()
    
    print("âš¡ PERFORMANCE TESTS:")
    print()
    print("   1. Response Time:")
    print("      â€¢ Single prediction < 100ms")
    print("      â€¢ Health check < 10ms")
    print("   ")
    print("   2. Throughput:")
    print("      â€¢ 100 requests < 2s")
    print("      â€¢ = 50 requests/second")
    print("   ")
    print("   3. Latency Percentiles:")
    print("      â€¢ P50 < 50ms")
    print("      â€¢ P95 < 200ms")
    print("      â€¢ P99 < 500ms")
    print()
    
    print("ðŸ’¡ WHY PERFORMANCE MATTERS:")
    print()
    print("   User expectations:")
    print("   â€¢ < 100ms: Instant")
    print("   â€¢ < 1s: Good")
    print("   â€¢ < 3s: Acceptable")
    print("   â€¢ > 3s: Frustrating")
    print()
    print("   Slow API:")
    print("   â€¢ Users leave")
    print("   â€¢ High bounce rate")
    print("   â€¢ Poor reviews")
    print()
    print("   Fast API:")
    print("   â€¢ Happy users âœ…")
    print("   â€¢ High engagement âœ…")
    print("   â€¢ Good reviews âœ…")
    print()


# ============================================================================
# 7. Testing with Fixtures
# ============================================================================

@pytest.fixture
def valid_request():
    """Fixture providing valid request data."""
    return {
        "features": [0.5, 0.6, 0.7],
        "model_version": "v1"
    }


@pytest.fixture
def invalid_request():
    """Fixture providing invalid request data."""
    return {
        "features": [],  # Empty
        "model_version": "v1"
    }


def test_with_valid_fixture(client, valid_request):
    """Test with valid request fixture."""
    response = client.post("/predict", json=valid_request)
    assert response.status_code == 200


def test_with_invalid_fixture(client, invalid_request):
    """Test with invalid request fixture."""
    response = client.post("/predict", json=invalid_request)
    assert response.status_code == 422


# ============================================================================
# Run Demonstrations
# ============================================================================

def main():
    """Run all demonstrations."""
    print("\nðŸ§ª Testing ML APIs\n")
    
    demo_why_test_apis()
    demo_testing_endpoints()
    demo_testing_errors()
    demo_testing_validation()
    demo_testing_performance()
    
    print("\n" + "=" * 70)
    print("âœ… Key Takeaways")
    print("=" * 70)
    print("""
1. Why Test APIs:
   - User-facing (bad API = bad UX)
   - Prevent 500 errors
   - Ensure fast responses
   - Catch validation errors

2. Endpoint Tests:
   âœ… Status codes (200, 422, 404, 500)
   âœ… Response format (JSON schema)
   âœ… Response types (float, str, etc.)
   âœ… Response ranges (0 â‰¤ confidence â‰¤ 1)

3. Error Handling:
   âœ… Missing fields â†’ 422
   âœ… Invalid types â†’ 422
   âœ… Out of range â†’ 422
   âœ… Server errors â†’ 500
   âœ… Invalid endpoint â†’ 404

4. Validation Tests:
   - Pydantic models enforce types
   - Field constraints (min/max)
   - Optional fields
   - Default values

5. Performance Tests:
   âœ… Response time < 100ms
   âœ… Health check < 10ms
   âœ… Throughput: 50+ req/sec
   âœ… Handle load testing

API Testing Checklist:
```
Endpoint Tests:
â–¡ Happy path (200 OK)
â–¡ Response format correct
â–¡ Response types correct
â–¡ Response ranges valid

Error Tests:
â–¡ Missing fields (422)
â–¡ Invalid types (422)
â–¡ Empty input (422)
â–¡ Out of range (422)
â–¡ Server errors (500)
â–¡ Invalid endpoints (404)

Validation Tests:
â–¡ Required fields enforced
â–¡ Optional fields work
â–¡ Type validation works
â–¡ Range validation works

Performance Tests:
â–¡ Response time < 100ms
â–¡ Health check < 10ms
â–¡ Handle 100 requests
â–¡ No memory leaks
```

FastAPI TestClient:
```python
from fastapi.testclient import TestClient

client = TestClient(app)
response = client.get("/health")
assert response.status_code == 200

response = client.post("/predict", json={...})
data = response.json()
assert "prediction" in data
```

Next Steps:
â†’ 05_data_validation.py (Validate input data)
â†’ integration_tests/ (End-to-end tests)
""")


if __name__ == "__main__":
    main()
